{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import pandas as pd\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_data = data[['reviews.rating', 'reviews.text', 'reviews.title']]\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare data\n",
    "def sentiment_labeler(score):\n",
    "    if (score==5) or (score==4):\n",
    "        return \"Positive\"\n",
    "    elif (score==3):\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Negative\"\n",
    "    \n",
    "sentiment_data[\"sentiment_label\"]=sentiment_data[\"reviews.rating\"].apply(sentiment_labeler)\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare train, test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split_data=StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n",
    "for tr_indx, ts_indx in split_data.split(sentiment_data, sentiment_data[\"reviews.rating\"]):\n",
    "    train_data=sentiment_data.reindex(tr_indx)\n",
    "    test_data=sentiment_data.reindex(ts_indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up stanfordcorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper function to extract pos tag features\n",
    "\n",
    "def extract_POS(statements):\n",
    "    print('Extracting POS Tags')\n",
    "    pos_tags = POS_tagging(statements,return_word_tag_pairs=False)\n",
    "    bigrams_pos = POS_groupping(pos_tags, grams=2)\n",
    "    trigrams_pos =POS_groupping(pos_tags, grams=3)\n",
    "    print('Finished')\n",
    "    return pos_tags,bigrams_pos,trigrams_pos\n",
    "\n",
    "##\n",
    "def POS_tagging(statements, return_word_tag_pairs = False):\n",
    "    core_nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "    print(\"NLP_Task ready to use.\")\n",
    "    POS_tags = list()\n",
    "    for statement in statements:\n",
    "        statement_tags = list()\n",
    "        annotations = core_nlp.annotate(statement, properties={\n",
    "            'annotators': 'tokenize,pos',\n",
    "            'outputFormat': 'json'\n",
    "            })\n",
    "        for output in annotations['sentences']:\n",
    "            statement_tags.append('<s>')\n",
    "            previous = ''\n",
    "            for token in output['tokens']:\n",
    "                if return_word_tag_pairs:\n",
    "                    statement_tags.append(token['word']+'/'+token['pos'])\n",
    "                else:\n",
    "                    statement_tags.append(token['pos'])\n",
    "\n",
    "        POS_tags.append(statement_tags)\n",
    "    return POS_tags\n",
    "\n",
    "## \n",
    "def POS_groupping(sentences_pos,grams=1):\n",
    "    result = list()\n",
    "    for sentence_tags in sentences_pos:\n",
    "        tag_group = list()\n",
    "        for index, each_tag in enumerate(sentence_tags):\n",
    "            if index < len(sentence_tags)-grams and len(sentence_tags)>=grams:\n",
    "                format_str = str()\n",
    "                for i in range(0,grams):\n",
    "                    format_str += sentence_tags[index+i]\n",
    "                    if i<grams-1:\n",
    "                        format_str += '_'\n",
    "                tag_group.append(format_str)\n",
    "        result.append(tag_group)\n",
    "    return result\n",
    "\n",
    "##\n",
    "def RemoveConsecutiveTags(list_to_remove, postags,ignore_punctuation=False):\n",
    "    withoutConsecutiveTags = list()\n",
    "    for each_tag in postags:\n",
    "        removed = list()\n",
    "        previous = ''\n",
    "        for tt in each_tag:\n",
    "            if tt != previous:\n",
    "                if not ignore_punctuation:\n",
    "                    removed.append(tt)\n",
    "                elif tt not in string.punctuation:\n",
    "                    removed.append(tt)\n",
    "                previous = tt\n",
    "            elif tt not in list_to_remove:\n",
    "                removed.append(tt)\n",
    "                previous = tt\n",
    "        withoutConsecutiveTags.append(removed)\n",
    "    return withoutConsecutiveTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extracting pos-tag features\n",
    "unigram_pos_tr, bigrams_pos_tr, trigram_pos_tr = extract_POS(statements=train_data['reviews.text'])\n",
    "unigram_pos_ts, bigrams_pos_ts, trigram_pos_ts = extract_POS(statements=test_data['reviews.text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove duplicated pos tag\n",
    "\n",
    "# For review data\n",
    "list_to_remove = ['NNP','CD']\n",
    "\n",
    "removed_pos_tr =RemoveConsecutiveTags(list_to_remove,unigram_pos_tr)\n",
    "removed_pos_bigrams_tr = POS_groupping(grams=2,sentences_pos=removed_pos_tr)\n",
    "removed_pos_trigrams_tr = POS_groupping(grams=3,sentences_pos=removed_pos_tr)\n",
    "\n",
    "removed_pos_ts =RemoveConsecutiveTags(list_to_remove,unigram_pos_ts)\n",
    "removed_pos_bigrams_ts = POS_groupping(grams=2,sentences_pos=removed_pos_ts)\n",
    "removed_pos_trigrams_ts = POS_groupping(grams=3,sentences_pos=removed_pos_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>pos_unig_tr</th>\n",
       "      <th>pos_big_tr</th>\n",
       "      <th>pos_trig_tr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>Batteries</td>\n",
       "      <td>NNS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>Great gift for any age. Wish I bought two. I c...</td>\n",
       "      <td>JJ NN IN DT NN .  NNP VBD CD .  PRP MD VB NN I...</td>\n",
       "      <td>&lt;s&gt;_JJ JJ_NN NN_IN IN_DT DT_NN NN_. ._&lt;s&gt; &lt;s&gt;_...</td>\n",
       "      <td>&lt;s&gt;_JJ_NN JJ_NN_IN NN_IN_DT IN_DT_NN DT_NN_. N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14084</th>\n",
       "      <td>My son loves this tablet. It is easy for him t...</td>\n",
       "      <td>PRPdollar NN VBZ DT NN .  PRP VBZ JJ IN PRP TO...</td>\n",
       "      <td>&lt;s&gt;_PRPdollar PRPdollar_NN NN_VBZ VBZ_DT DT_NN...</td>\n",
       "      <td>&lt;s&gt;_PRPdollar_NN PRPdollar_NN_VBZ NN_VBZ_DT VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8459</th>\n",
       "      <td>GREAT!</td>\n",
       "      <td>JJ .</td>\n",
       "      <td>&lt;s&gt;_JJ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24888</th>\n",
       "      <td>Very durable and easy to use. Last for several...</td>\n",
       "      <td>RB JJ CC JJ TO VB .  JJ IN JJ NNS IN DT IN PRP...</td>\n",
       "      <td>&lt;s&gt;_RB RB_JJ JJ_CC CC_JJ JJ_TO TO_VB VB_. ._&lt;s...</td>\n",
       "      <td>&lt;s&gt;_RB_JJ RB_JJ_CC JJ_CC_JJ CC_JJ_TO JJ_TO_VB ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  \\\n",
       "1394                                           Batteries   \n",
       "27998  Great gift for any age. Wish I bought two. I c...   \n",
       "14084  My son loves this tablet. It is easy for him t...   \n",
       "8459                                              GREAT!   \n",
       "24888  Very durable and easy to use. Last for several...   \n",
       "\n",
       "                                             pos_unig_tr  \\\n",
       "1394                                                 NNS   \n",
       "27998  JJ NN IN DT NN .  NNP VBD CD .  PRP MD VB NN I...   \n",
       "14084  PRPdollar NN VBZ DT NN .  PRP VBZ JJ IN PRP TO...   \n",
       "8459                                                JJ .   \n",
       "24888  RB JJ CC JJ TO VB .  JJ IN JJ NNS IN DT IN PRP...   \n",
       "\n",
       "                                              pos_big_tr  \\\n",
       "1394                                                       \n",
       "27998  <s>_JJ JJ_NN NN_IN IN_DT DT_NN NN_. ._<s> <s>_...   \n",
       "14084  <s>_PRPdollar PRPdollar_NN NN_VBZ VBZ_DT DT_NN...   \n",
       "8459                                              <s>_JJ   \n",
       "24888  <s>_RB RB_JJ JJ_CC CC_JJ JJ_TO TO_VB VB_. ._<s...   \n",
       "\n",
       "                                             pos_trig_tr  \n",
       "1394                                                      \n",
       "27998  <s>_JJ_NN JJ_NN_IN NN_IN_DT IN_DT_NN DT_NN_. N...  \n",
       "14084  <s>_PRPdollar_NN PRPdollar_NN_VBZ NN_VBZ_DT VB...  \n",
       "8459                                                      \n",
       "24888  <s>_RB_JJ RB_JJ_CC JJ_CC_JJ CC_JJ_TO JJ_TO_VB ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_feats_tr= pd.DataFrame()\n",
    "sentiment_feats_tr['reviews'] = train_data['reviews.text']\n",
    "\n",
    "list_to_remove = ['NNP','CD']\n",
    "sentiment_feats_tr['pos_unig_tr'] = [\" \".join(x).replace('<s>','').replace('$','dollar').strip() for x in removed_pos_tr]\n",
    "sentiment_feats_tr['pos_big_tr'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_bigrams_tr]\n",
    "sentiment_feats_tr['pos_trig_tr'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_trigrams_tr]\n",
    "sentiment_feats_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>pos_unig_tr</th>\n",
       "      <th>pos_big_tr</th>\n",
       "      <th>pos_trig_tr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18794</th>\n",
       "      <td>I debated on what tablet/reader to buy and dec...</td>\n",
       "      <td>PRP VBD IN WP JJR TO VB CC VBD IN DT NN .  PRP...</td>\n",
       "      <td>&lt;s&gt;_PRP PRP_VBD VBD_IN IN_WP WP_JJR JJR_TO TO_...</td>\n",
       "      <td>&lt;s&gt;_PRP_VBD PRP_VBD_IN VBD_IN_WP IN_WP_JJR WP_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190</th>\n",
       "      <td>First one out of the box I placed in a Swissph...</td>\n",
       "      <td>RB CD IN IN DT NN PRP VBD IN DT NN NN NN NN . ...</td>\n",
       "      <td>&lt;s&gt;_RB RB_CD CD_IN IN_IN IN_DT DT_NN NN_PRP PR...</td>\n",
       "      <td>&lt;s&gt;_RB_CD RB_CD_IN CD_IN_IN IN_IN_DT IN_DT_NN ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16684</th>\n",
       "      <td>im always using my amazon tablet...perfect for...</td>\n",
       "      <td>NN RB VBG PRPdollar NN NN : JJ IN NNS .</td>\n",
       "      <td>&lt;s&gt;_NN NN_RB RB_VBG VBG_PRPdollar PRPdollar_NN...</td>\n",
       "      <td>&lt;s&gt;_NN_RB NN_RB_VBG RB_VBG_PRPdollar VBG_PRPdo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4498</th>\n",
       "      <td>Never thought I would buy battery from amazon ...</td>\n",
       "      <td>RB VBN PRP MD VB NN IN JJ NN .  CC NN VBZ JJ ....</td>\n",
       "      <td>&lt;s&gt;_RB RB_VBN VBN_PRP PRP_MD MD_VB VB_NN NN_IN...</td>\n",
       "      <td>&lt;s&gt;_RB_VBN RB_VBN_PRP VBN_PRP_MD PRP_MD_VB MD_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19542</th>\n",
       "      <td>This tablet was the perfect gift for my husban...</td>\n",
       "      <td>DT NN VBD DT JJ NN IN PRPdollar NN WP VBP `` N...</td>\n",
       "      <td>&lt;s&gt;_DT DT_NN NN_VBD VBD_DT DT_JJ JJ_NN NN_IN I...</td>\n",
       "      <td>&lt;s&gt;_DT_NN DT_NN_VBD NN_VBD_DT VBD_DT_JJ DT_JJ_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  \\\n",
       "18794  I debated on what tablet/reader to buy and dec...   \n",
       "1190   First one out of the box I placed in a Swissph...   \n",
       "16684  im always using my amazon tablet...perfect for...   \n",
       "4498   Never thought I would buy battery from amazon ...   \n",
       "19542  This tablet was the perfect gift for my husban...   \n",
       "\n",
       "                                             pos_unig_tr  \\\n",
       "18794  PRP VBD IN WP JJR TO VB CC VBD IN DT NN .  PRP...   \n",
       "1190   RB CD IN IN DT NN PRP VBD IN DT NN NN NN NN . ...   \n",
       "16684            NN RB VBG PRPdollar NN NN : JJ IN NNS .   \n",
       "4498   RB VBN PRP MD VB NN IN JJ NN .  CC NN VBZ JJ ....   \n",
       "19542  DT NN VBD DT JJ NN IN PRPdollar NN WP VBP `` N...   \n",
       "\n",
       "                                              pos_big_tr  \\\n",
       "18794  <s>_PRP PRP_VBD VBD_IN IN_WP WP_JJR JJR_TO TO_...   \n",
       "1190   <s>_RB RB_CD CD_IN IN_IN IN_DT DT_NN NN_PRP PR...   \n",
       "16684  <s>_NN NN_RB RB_VBG VBG_PRPdollar PRPdollar_NN...   \n",
       "4498   <s>_RB RB_VBN VBN_PRP PRP_MD MD_VB VB_NN NN_IN...   \n",
       "19542  <s>_DT DT_NN NN_VBD VBD_DT DT_JJ JJ_NN NN_IN I...   \n",
       "\n",
       "                                             pos_trig_tr  \n",
       "18794  <s>_PRP_VBD PRP_VBD_IN VBD_IN_WP IN_WP_JJR WP_...  \n",
       "1190   <s>_RB_CD RB_CD_IN CD_IN_IN IN_IN_DT IN_DT_NN ...  \n",
       "16684  <s>_NN_RB NN_RB_VBG RB_VBG_PRPdollar VBG_PRPdo...  \n",
       "4498   <s>_RB_VBN RB_VBN_PRP VBN_PRP_MD PRP_MD_VB MD_...  \n",
       "19542  <s>_DT_NN DT_NN_VBD NN_VBD_DT VBD_DT_JJ DT_JJ_...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_feats_ts= pd.DataFrame()\n",
    "sentiment_feats_ts['reviews'] = test_data['reviews.text']\n",
    "\n",
    "list_to_remove = ['NNP','CD']\n",
    "sentiment_feats_ts['pos_unig_tr'] = [\" \".join(x).replace('<s>','').replace('$','dollar').strip() for x in removed_pos_ts]\n",
    "sentiment_feats_ts['pos_big_tr'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_bigrams_ts]\n",
    "sentiment_feats_ts['pos_trig_tr'] = [\" \".join(x).replace('$','dollar').strip() for x in removed_pos_trigrams_ts]\n",
    "sentiment_feats_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "def GetFeaturesFromPOS(training_data, user_defined_vocabulary=None):\n",
    "    user_defined_vocabulary = [x.lower().replace('$','dollar') for x in user_defined_vocabulary]\n",
    "\n",
    "    # making string of the data\n",
    "    training_str = [\" \".join(x) for x in training_data]\n",
    "\n",
    "    #replace $ by dollar\n",
    "    training_str = [x.replace('$', 'dollar').replace('<s>','sos') for x in training_str]\n",
    "\n",
    "    # features using binary iformation\n",
    "    oneHotVectorizer = CountVectorizer(vocabulary=user_defined_vocabulary,binary=True)\n",
    "    tr_onehot = oneHotVectorizer.fit_transform(training_str).toarray()\n",
    "    print(oneHotVectorizer.vocabulary_)\n",
    "\n",
    "    # features using no-binary information (counting)\n",
    "    countVectorizer = CountVectorizer(vocabulary=user_defined_vocabulary,binary=True)\n",
    "    tr_count = countVectorizer.fit_transform(training_str).toarray()\n",
    "\n",
    "    # features using tf-idf vectors\n",
    "    tfIdfVectorizer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "    tr_tfidf = tfIdfVectorizer.fit_transform(tr_count)\n",
    "\n",
    "    return tr_onehot, tr_count, tr_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vbz': 0, 'dt': 1, 'nnps': 2, 'vbp': 3, 'jj': 4, 'in': 5, 'wrb': 6, 'vbd': 7, 'prp': 8, 'rp': 9, 'wdt': 10, 'vb': 11, 'nnp': 12, 'vbg': 13, 'prpdollar': 14, 'vbn': 15, 'cd': 16, 'rb': 17, 'wp': 18, 'jjs': 19, 'jjr': 20, 'ex': 21, 'rbs': 22, 'fw': 23, 'ls': 24}\n",
      "{'nnps_vbp': 0, 'vb_nnp': 1, 'in_dt': 2, 'vb_jj': 3, 'jj_cd': 4, 'cd_nns': 5, 'dt_jjs': 6, 'jjr_in': 7, 'in_cd': 8, 'cc_in': 9, 'rb_vbd': 10, 'cd_nn': 11, 'nn_to': 12, 'jjr_jj': 13, 'vb_cd': 14}\n",
      "{'vbd_vbn_in': 0, 'in_dt_jj': 1, 'cd_nn_in': 2, 'in_cd_nns': 3, 'in_dt_nn': 4, 'dt_jj_cd': 5, 'md_vb_in': 6, 'jjs_jj_nn': 7, 'cc_jj_nns': 8, 'jj_nns_vbp': 9, 'vbp_cd_nn': 10, 'sos_jjr_in': 11, 'in_dt_nns': 12, 'jj_nn_md': 13}\n"
     ]
    }
   ],
   "source": [
    "pos_relevant_unigrams =  ['VBZ', 'DT', 'NNPS', 'VBP', 'JJ', 'IN', 'WRB', 'VBD', 'PRP', 'RP', 'WDT', 'VB', 'NNP', 'VBG', 'PRP$', 'VBN', 'CD', 'RB', 'WP', 'JJS', 'JJR', 'EX', 'RBS', 'FW', 'LS'] \n",
    "amazonRev_onehot_unigram_tr, amazonRev_count_unigram_tr, amazonRev_tfidf_unigram_tr = GetFeaturesFromPOS(training_data=removed_pos_tr, user_defined_vocabulary=pos_relevant_unigrams)\n",
    "\n",
    "pos_relevant_bigrams = ['NNPS_VBP', 'VB_NNP', 'IN_DT', 'VB_JJ', 'JJ_CD', 'CD_NNS', 'DT_JJS', 'JJR_IN', 'IN_CD', 'CC_IN', 'RB_VBD', 'CD_NN', 'NN_TO', 'JJR_JJ', 'VB_CD'] \n",
    "amazonRev_onehot_bigram_tr, amazonRev_count_bigram_tr, amazonRev_tfidf_bigram_tr = GetFeaturesFromPOS(training_data=removed_pos_bigrams_tr, user_defined_vocabulary=pos_relevant_bigrams)\n",
    "\n",
    "pos_relevant_trigrams = ['VBD_VBN_IN', 'IN_DT_JJ', 'CD_NN_IN', 'IN_CD_NNS', 'IN_DT_NN', 'DT_JJ_CD', 'MD_VB_IN', 'JJS_JJ_NN', 'CC_JJ_NNS', 'JJ_NNS_VBP', 'VBP_CD_NN', 'sos_JJR_IN', 'IN_DT_NNS','JJ_NN_MD']\n",
    "amazonRev_onehot_trigram_tr, amazonRev_count_trigram_tr, amazonRev_tfidf_trigram_tr = GetFeaturesFromPOS(training_data=removed_pos_trigrams_tr, user_defined_vocabulary=pos_relevant_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(amazonRev_onehot_unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(amazonRev_onehot_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(amazonRev_onehot_trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>pos_unig_tr</th>\n",
       "      <th>pos_big_tr</th>\n",
       "      <th>pos_trig_tr</th>\n",
       "      <th>pos_unigrams_1hot_tr</th>\n",
       "      <th>pos_bigrams_1hot_tr</th>\n",
       "      <th>pos_trigrams_1hot_tr</th>\n",
       "      <th>pos_unigrams_count_tr</th>\n",
       "      <th>pos_bigrams_count_tr</th>\n",
       "      <th>pos_trigrams_count_tr</th>\n",
       "      <th>pos_unigrams_tfidf_tr</th>\n",
       "      <th>pos_bigrams_tfidf_tr</th>\n",
       "      <th>pos_trigrams_tfidf_tr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>Batteries</td>\n",
       "      <td>NNS</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27998</th>\n",
       "      <td>Great gift for any age. Wish I bought two. I c...</td>\n",
       "      <td>JJ NN IN DT NN .  NNP VBD CD .  PRP MD VB NN I...</td>\n",
       "      <td>&lt;s&gt;_JJ JJ_NN NN_IN IN_DT DT_NN NN_. ._&lt;s&gt; &lt;s&gt;_...</td>\n",
       "      <td>&lt;s&gt;_JJ_NN JJ_NN_IN NN_IN_DT IN_DT_NN DT_NN_. N...</td>\n",
       "      <td>[0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0...</td>\n",
       "      <td>[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 1 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0...</td>\n",
       "      <td>[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 1 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>(0, 16)\\t0.42158413344508977\\n  (0, 14)\\t0.3...</td>\n",
       "      <td>(0, 2)\\t1.0</td>\n",
       "      <td>(0, 4)\\t1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14084</th>\n",
       "      <td>My son loves this tablet. It is easy for him t...</td>\n",
       "      <td>PRPdollar NN VBZ DT NN .  PRP VBZ JJ IN PRP TO...</td>\n",
       "      <td>&lt;s&gt;_PRPdollar PRPdollar_NN NN_VBZ VBZ_DT DT_NN...</td>\n",
       "      <td>&lt;s&gt;_PRPdollar_NN PRPdollar_NN_VBZ NN_VBZ_DT VB...</td>\n",
       "      <td>[1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0...</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0...</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>(0, 17)\\t0.3361587931606199\\n  (0, 14)\\t0.46...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8459</th>\n",
       "      <td>GREAT!</td>\n",
       "      <td>JJ .</td>\n",
       "      <td>&lt;s&gt;_JJ</td>\n",
       "      <td></td>\n",
       "      <td>[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>(0, 4)\\t1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24888</th>\n",
       "      <td>Very durable and easy to use. Last for several...</td>\n",
       "      <td>RB JJ CC JJ TO VB .  JJ IN JJ NNS IN DT IN PRP...</td>\n",
       "      <td>&lt;s&gt;_RB RB_JJ JJ_CC CC_JJ JJ_TO TO_VB VB_. ._&lt;s...</td>\n",
       "      <td>&lt;s&gt;_RB_JJ RB_JJ_CC JJ_CC_JJ CC_JJ_TO JJ_TO_VB ...</td>\n",
       "      <td>[0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0...</td>\n",
       "      <td>[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0...</td>\n",
       "      <td>[0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>[0 0 0 0 0 0 0 0 0 0 0 0 0 0]</td>\n",
       "      <td>(0, 17)\\t0.42721484019129746\\n  (0, 11)\\t0.4...</td>\n",
       "      <td>(0, 2)\\t1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 reviews  \\\n",
       "1394                                           Batteries   \n",
       "27998  Great gift for any age. Wish I bought two. I c...   \n",
       "14084  My son loves this tablet. It is easy for him t...   \n",
       "8459                                              GREAT!   \n",
       "24888  Very durable and easy to use. Last for several...   \n",
       "\n",
       "                                             pos_unig_tr  \\\n",
       "1394                                                 NNS   \n",
       "27998  JJ NN IN DT NN .  NNP VBD CD .  PRP MD VB NN I...   \n",
       "14084  PRPdollar NN VBZ DT NN .  PRP VBZ JJ IN PRP TO...   \n",
       "8459                                                JJ .   \n",
       "24888  RB JJ CC JJ TO VB .  JJ IN JJ NNS IN DT IN PRP...   \n",
       "\n",
       "                                              pos_big_tr  \\\n",
       "1394                                                       \n",
       "27998  <s>_JJ JJ_NN NN_IN IN_DT DT_NN NN_. ._<s> <s>_...   \n",
       "14084  <s>_PRPdollar PRPdollar_NN NN_VBZ VBZ_DT DT_NN...   \n",
       "8459                                              <s>_JJ   \n",
       "24888  <s>_RB RB_JJ JJ_CC CC_JJ JJ_TO TO_VB VB_. ._<s...   \n",
       "\n",
       "                                             pos_trig_tr  \\\n",
       "1394                                                       \n",
       "27998  <s>_JJ_NN JJ_NN_IN NN_IN_DT IN_DT_NN DT_NN_. N...   \n",
       "14084  <s>_PRPdollar_NN PRPdollar_NN_VBZ NN_VBZ_DT VB...   \n",
       "8459                                                       \n",
       "24888  <s>_RB_JJ RB_JJ_CC JJ_CC_JJ CC_JJ_TO JJ_TO_VB ...   \n",
       "\n",
       "                                    pos_unigrams_1hot_tr  \\\n",
       "1394   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
       "27998  [0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0...   \n",
       "14084  [1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0...   \n",
       "8459   [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
       "24888  [0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0...   \n",
       "\n",
       "                   pos_bigrams_1hot_tr           pos_trigrams_1hot_tr  \\\n",
       "1394   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
       "27998  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 1 0 0 0 0 0 0 0 0 0]   \n",
       "14084  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
       "8459   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
       "24888  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
       "\n",
       "                                   pos_unigrams_count_tr  \\\n",
       "1394   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
       "27998  [0 1 0 0 1 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 0...   \n",
       "14084  [1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 0...   \n",
       "8459   [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
       "24888  [0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0...   \n",
       "\n",
       "                  pos_bigrams_count_tr          pos_trigrams_count_tr  \\\n",
       "1394   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
       "27998  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 1 0 0 0 0 0 0 0 0 0]   \n",
       "14084  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
       "8459   [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
       "24888  [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]  [0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
       "\n",
       "                                   pos_unigrams_tfidf_tr pos_bigrams_tfidf_tr  \\\n",
       "1394                                                                            \n",
       "27998    (0, 16)\\t0.42158413344508977\\n  (0, 14)\\t0.3...          (0, 2)\\t1.0   \n",
       "14084    (0, 17)\\t0.3361587931606199\\n  (0, 14)\\t0.46...                        \n",
       "8459                                         (0, 4)\\t1.0                        \n",
       "24888    (0, 17)\\t0.42721484019129746\\n  (0, 11)\\t0.4...          (0, 2)\\t1.0   \n",
       "\n",
       "      pos_trigrams_tfidf_tr  \n",
       "1394                         \n",
       "27998           (0, 4)\\t1.0  \n",
       "14084                        \n",
       "8459                         \n",
       "24888                        "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_feats_tr['pos_unigrams_1hot_tr'] =  [str(x) for x in amazonRev_onehot_unigram_tr]\n",
    "sentiment_feats_tr['pos_bigrams_1hot_tr'] = [str(x) for x in amazonRev_onehot_bigram_tr]\n",
    "sentiment_feats_tr['pos_trigrams_1hot_tr'] = [str(x) for x in amazonRev_onehot_trigram_tr]\n",
    "\n",
    "sentiment_feats_tr['pos_unigrams_count_tr'] =  [str(x) for x in amazonRev_count_unigram_tr]\n",
    "sentiment_feats_tr['pos_bigrams_count_tr'] = [str(x) for x in amazonRev_count_bigram_tr]\n",
    "sentiment_feats_tr['pos_trigrams_count_tr'] = [str(x) for x in amazonRev_count_trigram_tr]\n",
    "\n",
    "sentiment_feats_tr['pos_unigrams_tfidf_tr'] =  [str(x) for x in amazonRev_tfidf_unigram_tr]\n",
    "sentiment_feats_tr['pos_bigrams_tfidf_tr'] = [str(x) for x in amazonRev_tfidf_bigram_tr]\n",
    "sentiment_feats_tr['pos_trigrams_tfidf_tr'] = [str(x) for x in amazonRev_tfidf_trigram_tr]\n",
    "\n",
    "sentiment_feats_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train different machine learning classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use different features\n",
    "\n",
    "# unigram_pos_str = [\" \".join(x) for x in unigram_pos]\n",
    "# bigram_pos_str = [\" \".join(x) for x in bigrams_pos]\n",
    "# trigram_pos_str = [\" \".join(x) for x in trigram_pos]\n",
    "\n",
    "# cv_uni = CountVectorizer()\n",
    "# pos_uni_feats = cv_uni.fit_transform(unigram_pos_str).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use unigram pos tagging as feature for training ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================\n",
      "accuracy metric and classification report in logistic regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy metrics for logistic regression classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.52      0.09      0.15       248\n",
      "     Neutral       0.00      0.00      0.00       185\n",
      "    Positive       0.91      1.00      0.95      4100\n",
      "\n",
      "    accuracy                           0.91      4533\n",
      "   macro avg       0.48      0.36      0.37      4533\n",
      "weighted avg       0.85      0.91      0.87      4533\n",
      "\n",
      "======================================================================================\n",
      "accuracy metric and classification report in SGD classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy metrics for stochastic gardient desent classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.59      0.04      0.08       248\n",
      "     Neutral       0.00      0.00      0.00       185\n",
      "    Positive       0.91      1.00      0.95      4100\n",
      "\n",
      "    accuracy                           0.91      4533\n",
      "   macro avg       0.50      0.35      0.34      4533\n",
      "weighted avg       0.85      0.91      0.86      4533\n",
      "\n",
      "======================================================================================\n",
      "accuracy metric and classification report in SVM\n",
      "accuracy metrics for support vector machine classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.96      0.09      0.16       248\n",
      "     Neutral       1.00      0.04      0.08       185\n",
      "    Positive       0.91      1.00      0.95      4100\n",
      "\n",
      "    accuracy                           0.91      4533\n",
      "   macro avg       0.96      0.38      0.40      4533\n",
      "weighted avg       0.92      0.91      0.87      4533\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pos_unigr_feats = sentiment_feats_tr['pos_unig_tr']\n",
    "# yTr=train_data[\"sentiment_label\"]\n",
    "# XTe=sentiment_feats_ts[\"reviews\"]\n",
    "# y_test=test_data[\"sentiment_label\"]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "pos_unigr_feats = cv.fit_transform(pos_unigr_feats).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(pos_unigr_feats,\n",
    "        train_data['sentiment_label'],train_size=0.8, random_state=123)\n",
    "\n",
    "print(\"======================================================================================\")\n",
    "print(\"accuracy metric and classification report in logistic regression\")\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR = LR.fit(X=X_train, y=y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(\"accuracy metrics for logistic regression classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"======================================================================================\")\n",
    "print(\"accuracy metric and classification report in SGD classifier\")\n",
    "\n",
    "\n",
    "sgd_clf = linear_model.SGDClassifier(max_iter=1000)\n",
    "sgd_clf=sgd_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=sgd_clf.predict(X_test)\n",
    "print(\"accuracy metrics for stochastic gardient desent classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"======================================================================================\")\n",
    "print(\"accuracy metric and classification report in SVM\")\n",
    "\n",
    "svm_clf=SVC(C=1, kernel='rbf', degree=3, gamma='auto', random_state=None)\n",
    "svm_clf=svm_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=svm_clf.predict(X_test)\n",
    "print(\"accuracy metrics for support vector machine classifier:\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using bigram pos tagging as features for training ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================\n",
      "accuracy metric and classification report in logistic regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy metrics for logistic regression classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.62      0.28      0.39       248\n",
      "     Neutral       0.29      0.02      0.04       185\n",
      "    Positive       0.92      0.99      0.95      4100\n",
      "\n",
      "    accuracy                           0.91      4533\n",
      "   macro avg       0.61      0.43      0.46      4533\n",
      "weighted avg       0.88      0.91      0.89      4533\n",
      "\n",
      "======================================================================================\n",
      "accuracy metric and classification report in SGD classifier\n",
      "accuracy metrics for stochastic gardient desent classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.60      0.25      0.36       248\n",
      "     Neutral       0.50      0.01      0.01       185\n",
      "    Positive       0.92      0.99      0.95      4100\n",
      "\n",
      "    accuracy                           0.91      4533\n",
      "   macro avg       0.67      0.42      0.44      4533\n",
      "weighted avg       0.88      0.91      0.88      4533\n",
      "\n",
      "======================================================================================\n",
      "accuracy metric and classification report in SVM\n",
      "accuracy metrics for support vector machine classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00       248\n",
      "     Neutral       0.00      0.00      0.00       185\n",
      "    Positive       0.90      1.00      0.95      4100\n",
      "\n",
      "    accuracy                           0.90      4533\n",
      "   macro avg       0.30      0.33      0.32      4533\n",
      "weighted avg       0.82      0.90      0.86      4533\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "pos_bigr_feats = sentiment_feats_tr['pos_big_tr']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "pos_bigr_feats = cv.fit_transform(pos_bigr_feats).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(pos_bigr_feats,\n",
    "        train_data['sentiment_label'],train_size=0.8, random_state=123)\n",
    "\n",
    "print(\"======================================================================================\")\n",
    "print(\"accuracy metric and classification report in logistic regression\")\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR = LR.fit(X=X_train, y=y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(\"accuracy metrics for logistic regression classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"======================================================================================\")\n",
    "print(\"accuracy metric and classification report in SGD classifier\")\n",
    "\n",
    "\n",
    "sgd_clf = linear_model.SGDClassifier(max_iter=1000)\n",
    "sgd_clf=sgd_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=sgd_clf.predict(X_test)\n",
    "print(\"accuracy metrics for stochastic gardient desent classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"======================================================================================\")\n",
    "print(\"accuracy metric and classification report in SVM\")\n",
    "\n",
    "svm_clf=SVC(C=1, kernel='rbf', degree=3, gamma='auto', random_state=None)\n",
    "svm_clf=svm_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=svm_clf.predict(X_test)\n",
    "print(\"accuracy metrics for support vector machine classifier:\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using trigram pos tagging as features for training ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================\n",
      "accuracy metric and classification report in logistic regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "c:\\users\\jvret\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy metrics for logistic regression classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.71      0.47      0.56       248\n",
      "     Neutral       0.63      0.34      0.44       185\n",
      "    Positive       0.95      0.99      0.97      4100\n",
      "\n",
      "    accuracy                           0.93      4533\n",
      "   macro avg       0.76      0.60      0.66      4533\n",
      "weighted avg       0.92      0.93      0.92      4533\n",
      "\n",
      "======================================================================================\n",
      "accuracy metric and classification report in SGD classifier\n",
      "accuracy metrics for stochastic gardient desent classifier:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.69      0.47      0.56       248\n",
      "     Neutral       0.61      0.34      0.43       185\n",
      "    Positive       0.95      0.98      0.96      4100\n",
      "\n",
      "    accuracy                           0.93      4533\n",
      "   macro avg       0.75      0.60      0.65      4533\n",
      "weighted avg       0.92      0.93      0.92      4533\n",
      "\n",
      "======================================================================================\n",
      "accuracy metric and classification report in SVM\n"
     ]
    }
   ],
   "source": [
    "pos_trigr_feats = sentiment_feats_tr['pos_trig_tr']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "pos_trigr_feats = cv.fit_transform(pos_trigr_feats).toarray()\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(pos_trigr_feats,\n",
    "        train_data['sentiment_label'],train_size=0.8, random_state=123)\n",
    "\n",
    "print(\"======================================================================================\")\n",
    "print(\"accuracy metric and classification report in logistic regression\")\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR = LR.fit(X=X_train, y=y_train)\n",
    "y_pred = LR.predict(X_test)\n",
    "print(\"accuracy metrics for logistic regression classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"======================================================================================\")\n",
    "print(\"accuracy metric and classification report in SGD classifier\")\n",
    "\n",
    "\n",
    "sgd_clf = linear_model.SGDClassifier(max_iter=1000)\n",
    "sgd_clf=sgd_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=sgd_clf.predict(X_test)\n",
    "print(\"accuracy metrics for stochastic gardient desent classifier:\\n\",classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"======================================================================================\")\n",
    "print(\"accuracy metric and classification report in SVM\")\n",
    "\n",
    "svm_clf=SVC(C=1, kernel='rbf', degree=3, gamma='auto', random_state=None)\n",
    "svm_clf=svm_clf.fit(X=X_train, y=y_train)\n",
    "y_pred=svm_clf.predict(X_test)\n",
    "print(\"accuracy metrics for support vector machine classifier:\\n\",classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTextBlobFeatures(corpus):\n",
    "    extractor = ConllExtractor()\n",
    "    text_blob_features = np.zeros((len(corpus),4))\n",
    "    blob_sentiment_analyzer = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "    for i,each_text in enumerate(corpus):\n",
    "        text_blob_features[i,0]=blob_sentiment_analyzer(each_text).sentiment[1]\n",
    "        text_blob_features[i,1]=blob_sentiment_analyzer(each_text).sentiment[2]\n",
    "        text_blob_features[i,2]= TextBlob(each_text).subjectivity\n",
    "        noun_phrase_extractor = TextBlob(each_text, np_extractor=extractor)\n",
    "        text_blob_features[i,3]= len(noun_phrase_extractor.noun_phrases)\n",
    "    return text_blob_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train_data['reviews.text']\n",
    "y_train=train_data[\"sentiment_label\"]\n",
    "X_test=test_data[\"reviews.text\"]\n",
    "y_test=test_data[\"sentiment_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr_tb_features = extractTextBlobFeatures(X_train)\n",
    "Xte_tb_features = extractTextBlobFeatures(X_test)\n",
    "ytr_tb_features = extractTextBlobFeatures(y_train)\n",
    "yte_tb_features = extractTextBlobFeatures(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtr= np.array(Xtr_tb_features.ravel(),)\n",
    "# ytr=np.array(ytr_tb_features.ravel(),)\n",
    "\n",
    "# Xte = np.array(Xte_tb_features.ravel(),)\n",
    "# yte = np.array(yte_tb_features.ravel(),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## use multinomial NB classifier\n",
    "# clf_multiNB_pipe = Pipeline([(\"vect\", CountVectorizer()), (\"tfidf\", TfidfTransformer()), (\"clf_nominalNB\", MultinomialNB())])\n",
    "# clf_multiNB_pipe.fit(Xtr, ytr)\n",
    "# predicted_nb = clf_multiNB_pipe.predict(Xte)\n",
    "# print(\"accuracy metrics for training naive bayes classifier:\\n\",metrics.classification_report(yte, predicted_nb, target_names = ['1','0','-1']))\n",
    "# print(\"======================================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
